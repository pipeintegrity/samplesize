---
title: "API 1163 Sample Size Determination"
author: "Center for Excellence in Risk Management"
date: "8/21/2020"
output: 
 bookdown::html_document2: 
  number_sections: false
  theme: journal
  toc: true 
  toc_float: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE)
```

## Introduction

In any type of sampling program it is a natural question to ask if enough samples have been taken to be able to discern the difference between the observations and some reference. This paper will discuss how to calculate a necessary sample size to be able to determine if the differences identified in the sampling program are significantly different than the specification or reference. In this context a sampling program can take the form of a material property confirmations or ILI digs to verify anomalies. Each property confirmation or anomaly measurement is a sample for this purpose.

Differences will always exist between the measured value and the true value since there is no such thing as an perfect measurement. Errors of varying magnitude will exist due to random effects from any number of sources, both human and equipment related. The t inevitable question arises is whether these observed differences are enough to be significant and have enough samples been completed to be able to tell the difference. Often tools such as a unity charts are used but even API 1163 refers to these as a qualitative tool. The weakness of unity charts is that they are only a visual representation of what is already known, how many anomalies were within the tolerance limits and what the difference was. It can't tell you what the likelihood that the entire population is within specification. Another scenario is when a series of replicate measurements are taken and because of the random errors inherent in any measuring process, there is scatter to the data even though there is only one true (but unknown) value for the sample, assuming the material property doesn't change from one measurement to the next. Even the average might be off from the expected value. If there is measurement uncertainty how confident can one be in the observed results are representative of reality and have enough samples been taken to reasonably judge whether a standard has been met?

To deal with the uncertainty built into any measuring process and sample variation, it is necessary to delve into the mathematics of probability to make inferences about the remaining population that was not examined. Statistics allows the user to take information from a sample to make inferences about the population.

## Statistical Tests and Errors

A sampling program is in essence a statistical test. In any statistical test there are two types of errors possible. They are known as Type 1 ($\alpha$) and Type 2 ($\beta$) errors. But the meaning of the two are often confused, both when setting up a testing program and when the results are interpreted. To help in the understanding of the implications, this paper will discuss the two types of error, what they mean. Also what will be discussed is how to set up a testing program and when to decide if the differences in the samples are significant.

In most cases, the desired result is to test whether the results from the sample data are different enough from a given reference or standard or between two samples that it is considered statistically significant. An example of this would be if a several wall thickness measurements are taken and the average is different than the assumed nominal wall thickness. A statistical test would be able to determine if the differences are enough to consider whether it was plausible that the sample could have came from a population with a mean equal to the nominal wall thickness.

In a Null Hypothesis Statistical Test (NHST) there are two possible outcomes. Either the Null Hypothesis (Null) which is your conjecture that you are attempting to disprove or not, (e.g. The mean of Sample A does not significantly differ from Sample B) is rejected or it is not rejected. It is not possible to accept the Null, it is either rejected or not rejected. The reason for this is that true value is unknown since only sample of the population was taken and accepting the Null would require proving an infinite number of alternatives are not possible, i.e. prove an infinite number of negatives. An NHST is similar to a criminal trial where the defendant is pronounced guilty or not guilty, but is never pronounced innocent.

## Discussion of Error Types

A Type 1 error is rejecting the Null when it is really true, remembering that the true value is unknown in most real world cases (except in a contrived examples) but we can calculate the likelihood of falsely rejecting the Null. A Type 2 error is failing to reject the Null when it is false, meaning you didn't reject the Null when you should have. The plot in figure \@ref(fig:type12) shows a Type 1 and 2 error. The darker shaded region is the error bounds for $\alpha$ and $\beta$ respectively. For instance, if a testing program is set up and it is decided that if the mean is equal or exceeds the value where the darker shaded ares begins that the the Null $H_0$ is rejected in favor of the alternate hypothesis $H_1$. It can be seen that there is still some possibility that the sample could have came from the Null because the distribution continues on. The potential Type 1 error would be the darker blue area under the curve. If at that same decision point, it was decided to not reject the Null that was false then the potential Type 2 error would be darker orange area.

```{r type12, fig.cap="Type 1 & 2 Errors"}

library(tidyverse)
library(patchwork)

label1 <- c("H[0]","H[1]")

DF <- data.frame (x=seq(-3,5,length.out = 301))

typeI <- ggplot(DF, aes(x)) +
  stat_function(
    fun = dnorm,
    geom = "area",
    fill = "steelblue",
    alpha = .3,
    n=301
  ) +
  stat_function(
    fun = dnorm,
    geom = "area",
    fill = "steelblue",
    xlim = c(qnorm(.95), 4)
  ) +
  stat_function(
    fun = dnorm,
    geom = "line",
    linetype = 2,
    col = "black",
    alpha = .5,
    args = list(
      mean = 2
    ),
    n=301
  ) +
  labs(
    title = "Type I Error",
    x = "z-score",
    y = "Density",
    caption = "Type I = Reject Null When it is True"
  ) +
  scale_x_continuous(limits = c(-3, 5))+
  annotate(geom = "text", x=0, y=0.3, label=deparse(bquote(H[0])), parse=T, size=5)+
  annotate(geom = "text", x=2, y=0.3, label=deparse(bquote(H[1])), parse=T, size=5)

typeII <- ggplot(DF, aes(x)) +
  stat_function(
    fun = dnorm,
    geom = "line",
    linetype = 2,
    n=301
  ) +
  stat_function(
    fun = dnorm,
    geom = "area",
    fill = "orange",
    args = list(
      mean = 2
    ),
    xlim = c(-2, qnorm(.95)),
    n=301
  ) +
  stat_function(
    fun = dnorm,
    geom = "area",
    linetype = 2,
    fill = "orange",
    alpha = .3,
    args = list(
      mean = 2
    ),
    n=301
  ) +
  labs(
    title = "Type II Error",
    x = "z-score",
    y = "Density",
    caption = "Type II = Failure to Reject Null When it is False"
  ) +
  scale_x_continuous(limits = c(-3, 5))+
  annotate(geom = "text",
           x=2, 
           y=0.3, 
           label=deparse(bquote(H[1])), parse=T, size=5)+
  annotate(geom = "text", 
           x=0, 
           y=0.3, 
           label=deparse(bquote(H[0])), parse=T, size=5)

typeI + typeII +
  plot_annotation(title = "Type I and Type II Error Visualization")

```

## Sample Size

There are two categories of tests that are used for sampling. The first one is if the test being used is a go/no-go, pass/fail criteria where there are only two possible outcomes. The second category is when a continuous value is measured such as yield strength or weight of some object. A continuous measure can take on an infinite number of values over some range and isn't limited to a pass/fail criteria. Each of these will be discussed in turn.

In the binary condition of the first criteria, the uncertainty in the test is not considered, only whether the sample is a pass or fail. In this type of test, the goal is to determine what proportion of the population is in one category or the other. This type of testing follows a binomial distribution which is used to calculate the likelihood of having *x* successes in *n* trials if the probability of success for each trial is *p*. In this context a "success" is whatever is being counted, so if the metric of interest is the number of samples that will likely fail then *p* is the proportion of the population that is hypothesized will fail or the maximum proportion allowed by some specification and *x* is the number of samples that did fail. In this context,"fail" does not mean that something physically failed but rather did not pass some test.

One of the principles of sampling is that the larger the sample the more faithful to the population it is. Intuitively, if only a couple samples are taken, the observed results might have happened purely due to chance and the population could be substantially different than the sample results. The opposite is also true, that if a very large sample is taken, the less likely the observed results are due to chance and the more likely the sample reflects the population. For example if a test was set up to determine if an unknown coin was fair, (equal chance of heads or tails) that only involved taking two samples (two flips). If both samples both came up heads and the experimenter rejects that the coin was fair, this would create significant doubt in the findings given the coin was only flipped twice. It is easily understood that these results could be purely due to chance. If on the other hand the coin is flipped several thousand times, the sample would definitely be representative of true proportion of heads to tails but the amount of time and effort were probably disproportionate to the amount of information gained after about fifty to a hundred flips.

The balance between these two extremes is where sample size determination comes in. If the cost of samples are of little consequence in terms of time or money, then more data can always be collected with minimal outlay. But in circumstances where a single sample might cost tens or hundreds of thousands of dollars and several weeks to collect, it is critical to determine the appropriate sample size to meet the goals of the sampling program and budget or schedule constraints.

So how many samples are required to show that something is significantly different from a reference proportion?

If the concern is about the likely tool error for anything left in the ground then it is important to understand that the amount of this error is not constant and will vary from location to location and even anomaly to anomaly at a given location. Because of this variation, the error is a distribution of probable error. This distribution will follow the Normal distribution which relies on a mean (average) and a standard deviation. In the this application it assumed that the average error is at or very near zero because the number of over and under calls will be be approximately the same. Because of the mean of zero (or very near), the problem of sample size becomes a question of the size of the likely standard deviation of the actual error relative to the tool tolerance specification. The standard deviation is a measure of the amount of "spread" or dispersion in a population.

Doing a number of digs is in a sense, analogous to an experiment where someone is taking $N$ repeated measurements to obtain a estimate of the standard deviation $\hat{\sigma}$ known as the sample standard deviation. The problem is how to choose an appropriate $N$ in such a way that I am able to be confident that the difference between the observed standard deviation $\hat{\sigma}$ does not vary from the true standard deviation $\sigma$ by more than some factor. The term "factor" is a deliberate one because the pertinent statistical distribution relies on the ratio of $\frac{\hat{\sigma}}{\sigma}$ which $\sigma$ is the true standard deviation. This distribution is related to not the Normal but a skewed distribution known as the *chi-square distribution*, denoted by $\chi^2$. More specifically its based on the ratio of the sampled variance to the true variance and the degrees of freedom. The variance is simply the square of the standard deviation, $\sigma^2$ and the degrees of freedom $n$ is the number of samples minus one $N-1$.

## Example Calculation

The chi-square distribution is defined by the relationship of equation \@ref(eq:chi).

```{=tex}
\begin{equation}
\frac{n\hat{\sigma}^2}{\sigma}
 (\#eq:chi)
\end{equation}
```
It can be shown that the ratio of $\frac{\hat{\sigma}}{\sigma}$ is contained with a level of probability equal to $\beta$, between the two quantities:\

```{=tex}
\begin{equation}
\frac{\chi_1^2}{n}\; and \; \frac{\chi^2_2}{n}
(\#eq:ratio)
\end{equation}
```
Where: $\chi^2_{1,2}$ are the critical values for $n$ degrees of freedom. The challenge becomes that both the numerator and denominator of the ratio are dependent on the degrees of freedom in \@ref(eq:ratio). Therefore there is no way to solve this directly. In years past it would necessitate looking up critical $\chi^2$ values in a table and find the one that matches your ratio and reference the degrees of freedom associated with it. But since computing power is cheap and plentiful it is convenient to create a list of $\frac{\chi^2}{n}$ this can be done in Excel by setting up a column with incremental degrees of freedom and calculate the inverse chi-square divided by degrees of freedom. This can be done with the *= CHISQ.INV()* function using the chosen significance level and the degrees of freedom column that was created previously, then this result is divided by that same degrees of freedom. The function requires a level of significance that has to be chosen by the user. This represents the limits of the level of confidence that is desired. For instance if a 5% significance is chosen, that implies that the level of confidence desired is 90%, from the 5% to 95% levels of the population, i.e. there is a 5% chance or less that the ratio of variances is being more extreme than what was calculated. Since we are interested in both tails of the distribution, the significance level at both ends of the distribution, at 5% and 95%.

In this example, a 95% confidence is desired so the level of significance would be $\frac{1-0.95}{2}=0.025$ for the lower end and $1-\frac{0.05}{2}=0.975$. In addition,it has been decided that a large enough sample needs to be taken such that you are able to estimate $\hat{\sigma}$ within 20% of the true value. Thus we require that:

```{=tex}
\begin{equation}
1-0.20<\frac{\hat{\sigma}}{\sigma}< 1+0.20
(\#eq:sdratio)
\end{equation}
```
Or

```{=tex}
\begin{equation}
  0.80<\frac{\hat{\sigma}}{\sigma}<1.20
  (\#eq:sdratio2)
  \end{equation}
```
Since the chi-square distribution is based on the ratio of variances the terms are all squared:

```{=tex}
\begin{equation}
  0.64<\frac{\hat{\sigma^2}}{\sigma^2}<1.44
  (\#eq:vratio)
  \end{equation}
```
To solve this, someone could enter the formula *=CHISQ.INV(n, 0.025)/n* into Excel with $n$ being a cell with the degrees of freedom and then by trial and error, changing $n$ until the formula produces a value of *not less than* 0.64 or another approach could create a column that iterates $n$ up to some large number and copy that formula down. Then find the closest value that meets the criteria. When that value is located in the list, the $n$ associated with that value is the degrees of freedom and since $n = N-1$ the necessary sample size is $n+1$. This same process is repeated for the upper end with *=CHISQ.INV(n, 0.975)/n* to find the one that is *not greater than* 1.44. In this example it works out to be 49 to satisfy both conditions. This will not always be the case, if the two values differ then select the larger of the two.

## Reverse Problem {.unnumbered}

In many cases digging to achieve a statistically significant sample is not feasible. In that case, the problem can be approached from the opposite direction. What is my uncertainty in the standard deviation based on a given number of digs? This problem is much more tractable than the previous and can be solved directly. Once again the chi-square distribution will be used but in a different manner.

If it were possible to take repeated samples of size $N$ and calculate the sample variance $\hat{\sigma^2}$ each time. Then the quantity $n\hat{\sigma^2}/{\sigma^2}$ has the chi-square distribution with $n$ degrees of freedom. Then we would write

```{=tex}
\begin{equation}
\frac{n\hat{\sigma^2}}{\sigma^2}=\chi_n^2
(\#eq:reverse)
\end{equation}
```
Where $\chi^2_n$ is the critical value for $n$ degrees of freedom. This is best illustrated with an example calculation.

## Sample Calculation for Reverse Problem {.unnumbered}

Let's assume a sample of 15 digs were completed and a sample standard deviation of the error between the ILI call and the field measurements is 9% (0.09) and a 90% certainty level is desired. The two critical values for $\chi^2$ can be calculated for $N-1$ degrees of freedom using the Excel formula *=CHISQ.INV(14, 0.05)* & *=CHISQ.INV(14, 0.95)* which would give the values of 6.5706 and 23.6847. Using equation \@ref(eq:reverse) it can be inferred that the ratio of the variances is $\frac{\hat{\sigma^2}}{\sigma^2}=\frac{\chi^2_n}{n}$. Therefore the ratio of the variances will lie between 6.5706/14 and 23.6847/14 and thus variance ratio, $0.4693< \frac{\hat{\sigma^2}}{\sigma^2}<1.6918$. Knowing that the standard deviation is the square root of the variance nd rearranging the inequality you get $\hat{\sigma}/1.3007\;<\;\sigma\;<\;\hat{\sigma}/0.6851$. In the problem statement it was given that $\hat\sigma=0.09$ so the true standard deviation will be between 6.9% and 13.1% with a 90% probability.

## Confidence Intervals, Confidence Levels and Tolerance Limits {.unnumbered}

One of the most misused terms in statistics is "Confidence Interval". This is often incorrectly used in the context that it tells the user the limits of uncertainty for some measurement with a stated confidence. A confidence interval is a statement about the precision of some parameter, typically the mean. It is the plausible range for the parameter that will contain the true value with some chosen confidence level. To illustrate this, think of a very large population of something with some parameter of interest, such as the length or depth. Then you take a random sample from that population and calculate the confidence interval for that sample and then put the sample back and take another random sample and calculate a confidence interval again. This process is repeated potentially hundreds or thousands of times,each time calculating a confidence interval. As the number of samples increases, 95% of the confidence intervals will contain the true mean of the population. But a 95% confidence interval does not imply that 95% of the samples will be within the limits of the confidence interval. The interpretation of a 95% confidence interval would be that we are 95% confident that the true value lies between the upper and lower confidence interval.

The confidence level is a chosen parameter based on the level of certainty desired. The larger the confidence level, the wider the confidence interval will be.

The third item is the tolerance limits, this is a characteristic of the population and is often what is really being referred to when the term confidence interval is used. This is the expected limits (or interval) that a specified proportion of the population will lie with some confidence level. If the true mean and standard deviation of the population were known then the expected bounds would simply be the probability interval of the Normal distribution for the desired confidence level. However with the exception of the more trivial cases where the population is small and 100% can be examined, the inference is based on a sample of the population. Since only the mean and standard deviation of the sample is known and not the population, some level of uncertainty exists about the true value of the parameters. The following example will demonstrate how to calculate the tolerance limits.

## Tolerance Limit Example {.unnumbered}

In this example, the desired confidence level ($\alpha$) is 95% that 80% ($p$) of the anomalies will be within the tolerance limits. After examining a sample of 20 anomaly depths, a standard deviation of 9% ($\hat{\sigma}$) and a mean of - 1% ($\bar{x}$) (Undercalls being negative) of the difference between what was reported by the ILI tool and what was found were calculated. The tolerance limits based on this sample is calculated as follows:

```{=tex}
\begin{equation}
X_{L,U} = \bar{x}\pm k_2\hat{\sigma}
(\#eq:tolerance)
\end{equation}
```
And:

```{=tex}
\begin{equation}
k_2 = z_{\frac{1+p}{2}}\sqrt{ \frac{n\left(1+\frac{1}{N}\right)}{\chi^2_{1-\alpha,n}}}
(\#eq:kfactor)
\end{equation}
```
Where: $X_{L,U}$ = the Lower and Upper Tolerance Limit respectively\
$z_{\frac{1+p}{2}}$ = The critical value of the Normal Distribution for a probability of $\frac{1+p}{2}$. For p = 0.80, z = 1.28\
$N$ = Sample Size\
$n$ = Degrees of Freedom = $N-1$\
$\chi^2$ = Critical Chi-Square value as discussed above, for $n = 19$ and $1-\alpha = 0.05$, $\chi^2 = 10.12$

Typically the amount of potential undercall for any unexamined anomalies is of the greatest consequence and the lower limit of the tolerance interval would be of the most interest. But for the sake of demonstration, the upper and lower limits are calculated.

```{=tex}
\begin{equation}
k_2 = 1.28\sqrt{ \frac{19\left(1+\frac{1}{20}\right)}{10.12}} = 1.80  
(\#eq:kfactorex)
\end{equation}
```
The tolerance limits would be as follows:

```{=tex}
\begin{equation}
X_L = -0.01- (1.80)(0.09) = -17\% \hspace{12pt} and \hspace{12pt}  X_U = -0.01+ (1.80)(0.09) = 15\%
(\#eq:tolerancex)
\end{equation}
```
## Binary Outcomes {.unnumbered}

In the condition where each sample is a pass/fail, yes/no type of binary outcome then that is modeled with the binomial distribution. This is used to calculate the probability of *k* successes in *n* independent trials if the probability of success per trial is *p*. A "success" is whatever is being counted. A success does not necessarily a positive outcome. The binomial distribution is as follows.

$$\binom{n}{k} p^{k} (1 - p)^{n - k}$$

Where the $\binom{n}{k}$ is what is called the binomial coefficient which calculates the number of ways *k* successes in *n* trials can be distributed. It would be easy to show the probability of observing a given number of successes in a number of trials, but if the sampling is to intended to show that something is very unlikely then this doesn't show how likely someone is to find something if additional samples were taken. For instance, the probability of zero successes in one trial is 95% if the inconsistent material is 5% of the population. But even the casual observer would realize that doesn't prove anything and the result of one trial could be purely the result of random chance. That's equivalent to saying 95% of all cars are white because the first one you saw was white. To show the probability of **not** finding something in future trials it is necessary to calculate the opposite, the probability of finding **one or more** successes in n trials as shown in Figure \@ref(fig:cum-prob) where the *N* is the number of trials it would take to show with a 95% certainty of finding one or more successes if the percent of the population was *P*. The cumulative probability of finding one or more is simply one minus the probability of finding zero successes in *n* trials. This calculation is based on the assumption that population that is being sampled from is homogeneous and each trial is independent of each other.

```{r cum-prob, fig.cap="Probability of One or More Successes", fig.width=6.5, fig.asp=3/4}

library(tidyverse)

size <- 5:75
prob <- c(0.03, 0.05, 0.10, 0.15)


crossed <- crossing(size, prob) %>%
  mutate(p_s = pbinom(q = 0,size = size,prob = prob,lower.tail = F))

min_samp <- crossed %>%
  filter(p_s >= 0.95) %>%
  group_by(prob) %>%
  summarise(mins =min(size)) %>%
  mutate(p = pbinom(q = 0,size = mins,prob = prob,lower.tail = F))


crossed %>%
  ggplot(aes(size, p_s)) +
  # geom_point(alpha = 0.6, aes(col = factor(prob))) +
  geom_line(aes(col = factor(prob)), lwd = 0.9) +
  scale_y_continuous(
    breaks = scales::pretty_breaks(),
    labels = scales::label_percent(),
    limits = c(0.1, 1),
    expand = c(0,0)
  ) +
  scale_x_continuous(breaks = seq(0, 75, by = 10)) +
  geom_hline(
    lty = 2,
    col = 'black',
    yintercept = 0.95,
    lwd = 0.8
  ) +
  geom_segment(data = min_samp,
               aes(
                 x = mins,
                 xend = mins,
                 y = 0.1,
                 yend = p,
                 col = factor(prob)
               ),
               lty = 2,
               lwd=0.8
               ) +
  geom_text(data = min_samp,
            aes(
              x = mins,
              y = 0.25,
              label = paste("P =", prob, "\nN =", mins)
            ),
            size = 5,
            angle =-90,

            ) +
  theme_bw(12) +
  labs(
    title = "Number of Tirals to Confirm 95% Certainty",
    color = "Population Percent",
    y = "Probability of One or More",
    x = "Number of Trials"
  )+
  theme(legend.position = "bottom")

```

## Conclusion {.unnumbered}

Sample size and the uncertainty based on a fixed sample size can easily be computed using a spreadsheet or any statistical software. It should be cautioned that the larger the level of certainty to be attained or the smaller the deviation from the true standard deviation that is being attempting to discern, the larger the required sample will be. This number can exceed several hundred for the extremes of the two inputs. Total certainty and zero risk does not exist when dealing with sample statistics therefore the engineer should decide what is the greatest amount uncertainty that can be accepted that will lead to an acceptable risk level for an ILI run. But using the lower tolerance limits and the as-called dimensions it is now possible to calculate the probable lower end of predicted failure pressure and then make an informed decision if that is acceptable.
